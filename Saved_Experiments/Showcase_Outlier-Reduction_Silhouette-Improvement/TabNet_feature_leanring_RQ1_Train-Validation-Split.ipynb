{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6e817a",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "- Use KL divergence in custom loss function\n",
    "- determin number of bins for distributions using Freedman-Diaconis rule\n",
    "- added test_size to config\n",
    "- now using CUDA (GPU) instead of CPU (if NVIDIA Cuda available)\n",
    "- save output data and models for multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf513c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a78625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() + \"/pytorch_tabnet/\")\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.special import rel_entr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4886e95",
   "metadata": {},
   "source": [
    "## 0. Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3638a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should an output folder be created to save the experiment data?\n",
    "save_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9f91a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read config file\n",
    "import toml\n",
    "config = toml.load(\"config.toml\")\n",
    "\n",
    "# rnd seed to support reproducable results\n",
    "seed_value =  config[\"general\"][\"random_seed\"] if config[\"general\"][\"random_seed\"] else random.randrange(sys.maxsize)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# TabNet hyperparameters\n",
    "regressor_param = {\"optimizer_fn\" : torch.optim.Adam,\n",
    "                   \"optimizer_params\" : {\"lr\":0.05},\n",
    "                   \"scheduler_params\" : {\"step_size\": 2, \"gamma\": 0.9},\n",
    "                   \"scheduler_fn\" : torch.optim.lr_scheduler.StepLR,\n",
    "                   \"mask_type\" : 'entmax',\n",
    "                   \"device_name\" : 'cuda', # only working on NVIDIA GPUs\n",
    "                  }\n",
    "\n",
    "if save_output:\n",
    "    # create experiment folder\n",
    "    from datetime import datetime\n",
    "    \n",
    "    experiment_name = config[\"general\"][\"experiment_name\"]\n",
    "    experiment_folder = f\"{experiment_name} {datetime.now().strftime('%Y_%m_%d %H-%M-%S')}\" if experiment_name else f\"Exp {datetime.now().strftime('%Y_%m_%d %H-%M-%S')}\"\n",
    "    \n",
    "    experiment_path = os.path.join(\"Experiments\", experiment_folder)\n",
    "    os.mkdir(experiment_path)\n",
    "    \n",
    "    plots_path = os.path.join(experiment_path, \"plots\")\n",
    "    os.mkdir(plots_path)\n",
    "    \n",
    "    models_path = os.path.join(experiment_path, \"models\")\n",
    "    os.mkdir(models_path)\n",
    "    \n",
    "    # save config file\n",
    "    import shutil\n",
    "    shutil.copy2(\"config.toml\", experiment_path)\n",
    "    \n",
    "    # save random seed\n",
    "    if not config[\"general\"][\"random_seed\"]:\n",
    "        config[\"general\"][\"random_seed\"] = seed_value\n",
    "        config_path = os.path.join(experiment_path, \"config.toml\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            toml.dump(config, f)\n",
    "    \n",
    "    # save model hyperparameters\n",
    "    with open(os.path.join(experiment_path, 'config.toml'), 'a') as config_file:\n",
    "        config_file.write(\"\\n#[regressor_param]\\n\")\n",
    "        for key, value in regressor_param.items(): \n",
    "            config_file.write('#%s = %s\\n' % (key, str(value)))\n",
    "    \n",
    "    # save notebook\n",
    "    shutil.copy2(nb_name, experiment_path)#\n",
    "    \n",
    "    # create output file\n",
    "    output_path = os.path.join(experiment_path, \"output.toml\")\n",
    "    output_file = open(output_path, \"w\")\n",
    "    output = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955edf3b",
   "metadata": {},
   "source": [
    "## 1. Datasets\n",
    "\n",
    "Different datasets are to be used for the experiments.\n",
    "- toy dataset using make_blobs from sklearn mainly to setup and test the notebook\n",
    "- DAMI Benchmark datasets with the respective best practice hyperparameters (https://www.dbs.ifi.lmu.de/research/outlier-evaluation/DAMI/)\n",
    "\n",
    "### 1.0. Create toy dataset\n",
    "As of now, only a toy dataset is used (n_features features with clusters centered around different points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "features_arr, clusters_arr = make_blobs(**config[\"toy_data\"])\n",
    "\n",
    "# name feature and cluster (ground truth) columns\n",
    "features = pd.DataFrame(features_arr, columns=[f\"F{x + 1}\" for x in range(features_arr.shape[1])])\n",
    "clusters = pd.DataFrame(clusters_arr, columns=[\"C\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640314ab",
   "metadata": {},
   "source": [
    "### 1.1. plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8dbe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dataset\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(features_arr[:, 0], features_arr[:, 1], features_arr[:, 2], c=clusters_arr)\n",
    "plt.gca().update(dict(title='DBSCAN with reconstructed data', xlabel='F1', ylabel='F2', zlabel='F3'))\n",
    "plt.show()\n",
    "if save_output:\n",
    "    plt.savefig(os.path.join(plots_path, \"0_ground_truth_clusters.png\"), **config[\"plots\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2399f7f",
   "metadata": {},
   "source": [
    "### 1.2. Create missing data\n",
    "Create a specified amount of missing values in the given feature columns (for complete datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create missing values in feature column(s)\n",
    "mv_config = {feature:mv_ratio for feature,mv_ratio in zip(list(features.columns), config[\"general\"][\"missing_percent\"])}\n",
    "features_mv = features.copy()\n",
    "\n",
    "for f, mv in mv_config.items():\n",
    "    features_mv[f] = features_mv[f].mask(np.random.random(features.shape[0]) < mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aca2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "%matplotlib inline\n",
    "\n",
    "# plot missing values\n",
    "fig = msno.matrix(features_mv)\n",
    "plt.show()\n",
    "if save_output:\n",
    "    #plt.savefig(os.path.join(plots_path, \"missing_values.png\"), **config[\"plots\"])\n",
    "    fig_copy = fig.get_figure()\n",
    "    fig_copy.savefig(os.path.join(plots_path, \"missing_values.png\"), **config[\"plots\"], bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2954a63",
   "metadata": {},
   "source": [
    "### 1.3. Initial Clustering (without missing values)\n",
    "Use the cluster algorithm on dataset without missing values.\\\n",
    "Get cluster means for loss function input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DBSCAN on original data\n",
    "dbscan_orig = DBSCAN(**config[\"DBSCAN\"]).fit(features)\n",
    "clusters_orig = dbscan_orig.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clustering\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(features.iloc[:, 0], features.iloc[:, 1], features.iloc[:, 2], c=clusters_orig)\n",
    "plt.gca().update(dict(title='DBSCAN with reconstructed data', xlabel='F1', ylabel='F2', zlabel='F3'))\n",
    "plt.show()\n",
    "if save_output:\n",
    "    plt.savefig(os.path.join(plots_path, \"1_DBSCAN_original_data.png\"), **config[\"plots\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster means (used to calculate the cluster mean loss)\n",
    "complete_data = features.copy()\n",
    "complete_data[\"C\"] = clusters_orig\n",
    "cluster_means = complete_data.groupby(\"C\").agg(\"mean\")\n",
    "cluster_means = cluster_means if -1 not in cluster_means.index else cluster_means.drop(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96864266",
   "metadata": {},
   "source": [
    "## 2. Filling Strategies\n",
    "\n",
    "\n",
    "### 2.1. Define custom-loss\n",
    "The loss function for the model fitting shall be customized to improve cluster perforamnce. The custom loss is divided into 2 separate losses\n",
    "\n",
    "1. **Reconstruction Loss**\\\n",
    "Classic loss function, such as RMSE.\n",
    "2. **Cluster-Mean Loss**\\\n",
    "The distance to the nearest cluster mean shall be minimized.\n",
    "3. **KL Divergence Loss**\\\n",
    "Deviation of original distribution shall be minimized \\\n",
    "(prob distribution based on https://stackoverflow.com/questions/57687458/how-to-calculate-probabilities-using-numpy-histogram-and-then-use-it-for-calcula \\\n",
    "number of bins: D. Freedman & P. Diaconis (1981) “On the histogram as a density estimator: L2 theory”. Probability Theory and Related Fields 57 (4): 453-476)\n",
    "\n",
    "The chosen approach to implement the cluster-mean loss is as follows:\\\n",
    "**Adapt the loss function** to calculate a distance to each cluster. The minimum distance (to the nearest cluster) will be used as a cluster-mean-loss. Minimizing this loss should result in the data point moving closer to the nearest cluster mean.\\\n",
    "Potential Problem: loss functions currently work with y_true (true values) and y_score (predicted values). Calculating the distance would require all features. Calculating only the 1 dimensional distance will result in missing values being put into wrong clusters - depending on the first estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom loss function: extend L2-loss by cluster-mean-loss\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import astropy.stats\n",
    "\n",
    "class custom_loss(Metric):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"custom_loss\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score, cluster_means, target_feature, x_val, y_val):\n",
    "        # reconstruction loss\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_score))\n",
    "        \n",
    "        # cluster mean loss\n",
    "        cml = self.cluster_mean_loss(cluster_means, y_score, target_feature, x_val, y_val)\n",
    "        \n",
    "        # KL divergence loss\n",
    "        bins = len(astropy.stats.freedman_bin_width(y_true.reshape(1,-1)[0], return_bins=True)[1])\n",
    "        distr1, distr2 = self.probability_dist(y_score, y_true, bins)\n",
    "        KLdl = torch.nn.functional.kl_div(torch.tensor(distr1).log(), torch.tensor(distr2), log_target=False, reduction='sum').item()\n",
    "        \n",
    "        # total loss\n",
    "        custom_loss = rmse + KLdl + cml\n",
    "        return custom_loss\n",
    "    \n",
    "    # calculate sum of distances to the closest cluster mean\n",
    "    def cluster_mean_loss(self, cluster_means, y_score, target_feature, x_val, y_val):\n",
    "        \n",
    "        # reorder cluster_means to match x_val columns (predicted feature is last column)\n",
    "        cols = cluster_means.columns.tolist()\n",
    "        feature_col = features.columns.get_loc(target_feature)\n",
    "        cols = cols[:feature_col] + cols[feature_col+1:] + [target_feature]\n",
    "        \n",
    "        # x values extended by predicted values\n",
    "        x_extended = pd.DataFrame(np.hstack((x_val, y_score)), columns=cols)\n",
    "\n",
    "\n",
    "        # calculate distances to each cluster and select the closest one\n",
    "        dist = []\n",
    "        for cluster in range(cluster_means.shape[0]):\n",
    "            dist.append(np.linalg.norm((x_extended - cluster_means.iloc[cluster]), axis=1))\n",
    "            \n",
    "        dist = pd.DataFrame(dist).T\n",
    "        min_dist = dist.assign(min_dist=lambda d: d.min(1))[\"min_dist\"]\n",
    "        \n",
    "        return sum(min_dist)\n",
    "    \n",
    "    # calculate probabilities\n",
    "    def probability_dist(self, x, y, bins):\n",
    "        # transforms 2 series into their probability distributions\n",
    "        # histogram\n",
    "        hist_xy = np.histogram2d(np.asarray(x)[:,0], np.asarray(y)[:,0], bins=bins)[0]\n",
    "\n",
    "        # compute marginals\n",
    "        hist_xy = hist_xy + sys.float_info.min # prevent division with 0\n",
    "        hist_xy = hist_xy / np.sum(hist_xy)\n",
    "        hist_x = np.sum(hist_xy, axis=1)\n",
    "        hist_y = np.sum(hist_xy, axis=0)\n",
    "        return(hist_x, hist_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5da4a8",
   "metadata": {},
   "source": [
    "### 2.2. Fill missing values (TabNet).\n",
    "\n",
    "**Approach 1:**\\\n",
    "For each feature that contains missing values, use all rows with complete other features as train set and predict missing values of the feature.\\\n",
    "&rarr; Problem: lots of models, especially for high-dimensional datasets. \n",
    "Also problem, if more than one feature is missing values. &rarr; see Approach 2\n",
    "\n",
    "**Approach 2:**\\\n",
    "Mask missing values as -1 (or some other value) and fit a model predicting each feature just like in approach 1.\\\n",
    "&rarr; one model for each feature containing missing values.\\\n",
    "Possible adaption: use predicted values to predict subsequent values (instead of using -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_to_predict = features_mv.columns[features_mv.isnull().any()]\n",
    "features_reconstr = []\n",
    "costs = {}\n",
    "output[\"runtimes\"] = {}\n",
    "\n",
    "for run in range(config[\"RQ2\"][\"runs\"]):\n",
    "    time_start = time.time()\n",
    "    features_reconstr.append(features_mv.copy())\n",
    "    costs[run] = []\n",
    "    for feature in features_to_predict:\n",
    "        print(f\"Run {run+1}, predicting {feature}\")\n",
    "\n",
    "        # select % of features for prediction based on pred_features (e.g. 10% of randomly selected features)\n",
    "        number_of_features = math.ceil(config[\"RQ2\"][\"pred_features\"] * (len(features_mv.columns)) - 1)\n",
    "        features_mv_masked = features_mv.copy().drop(feature, axis=1).sample(n=number_of_features, axis='columns')\n",
    "        selected_features = features_mv_masked.columns\n",
    "        \n",
    "        print(f\"Predictions based on features {list(selected_features)}\")\n",
    "        \n",
    "        # mask other columns' missing values as defined in config (missing_value_mask)\n",
    "        features_mv_masked[feature] = features_mv[feature]\n",
    "        features_mv_masked[selected_features] = features_mv_masked[(selected_features)].fillna(config[\"general\"][\"missing_value_mask\"])\n",
    "        \n",
    "        # separate complete rows as train data\n",
    "        # separate column with missing values as target\n",
    "        test_data = features_mv_masked[features_mv_masked[feature].isna()].drop(feature, axis=1).to_numpy()\n",
    "        train_data = features_mv_masked[features_mv_masked[feature].notna()]\n",
    "        train_features = train_data.drop(columns=feature).to_numpy()\n",
    "        train_target = train_data[feature].to_numpy().reshape(-1,1)\n",
    "\n",
    "        x_train, x_val, y_train, y_val = train_test_split(train_features, train_target, test_size=config[\"training\"][\"test_size\"])\n",
    "\n",
    "        # create & train model\n",
    "        model = TabNetRegressor(**regressor_param)\n",
    "        model.fit(\n",
    "            x_train, y_train,\n",
    "            eval_set=[(x_val, y_val)],\n",
    "            eval_metric=['custom_loss'],\n",
    "            **config[\"model_param\"],\n",
    "            cluster_means=cluster_means,\n",
    "            target_feature=feature,\n",
    "            x_val=x_val,\n",
    "            y_val=y_val,\n",
    "        )\n",
    "\n",
    "        #track runtime\n",
    "        time_end = time.time()\n",
    "        \n",
    "        # add costs to dict\n",
    "        costs[run].append(model.best_cost)\n",
    "        \n",
    "\n",
    "        # fill missing values with model predictions\n",
    "        features_reconstr[run].loc[features_reconstr[run][feature].isna(), feature] = [x[0] for x in model.predict(test_data)]\n",
    "\n",
    "        # save outputs\n",
    "        if save_output:\n",
    "            \n",
    "            # save model\n",
    "            torch.save(model, os.path.join(models_path, f\"Model_run{run}_{feature}.pt\"))\n",
    "            \n",
    "            # save runtimes\n",
    "            output[\"runtimes\"][f\"run{run}_{feature}\"] = time_end - time_start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7f704",
   "metadata": {},
   "source": [
    "## 3. Complete Dataset\n",
    "Costs for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56dc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3130b",
   "metadata": {},
   "source": [
    "### 3.1 Reconstruct Dataset\n",
    "choose best model based on min. squared custom loss (punishing large outliers in losses for single features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44685a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_totals = [sum([x**2 for x in c]) for _, c in costs.items()]\n",
    "best_model = cost_totals.index(min(cost_totals))\n",
    "features_best = features_reconstr[best_model]\n",
    "print(f\"Best model: run {best_model} with custom losses {costs[best_model]} for features {list(features_to_predict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a53882",
   "metadata": {},
   "source": [
    "### 3.2 Plot changes (comp. to original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad12bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot with combined data\n",
    "orig_data = features.copy()\n",
    "orig_data[\"change\"] = 0\n",
    "changed_data = features_best[features_mv.isna().any(axis=1)]\n",
    "comp_data = pd.concat([orig_data, changed_data]).reset_index(drop=True).fillna(1)\n",
    "\n",
    "# line plot connecting filled data to true data points\n",
    "reconstr_data = features_best[features_mv.isna().any(axis=1)]\n",
    "true_data = features[features_mv.isna().any(axis=1)]\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax  = fig.add_subplot(projection = '3d')\n",
    "ax.scatter(comp_data[\"F1\"], comp_data[\"F2\"], comp_data[\"F3\"], c=[comp_data[\"change\"]])\n",
    "\n",
    "for i in range(len(reconstr_data)):\n",
    "    ax.plot([reconstr_data[\"F1\"].iloc[i], true_data[\"F1\"].iloc[i]],[reconstr_data[\"F2\"].iloc[i], true_data[\"F2\"].iloc[i]],[reconstr_data[\"F3\"].iloc[i], true_data[\"F3\"].iloc[i]], color=\"black\", linewidth=.5)\n",
    "\n",
    "\n",
    "plt.gca().update(dict(title='DBSCAN with reconstructed data', xlabel='F1', ylabel='F2', zlabel='F3'))\n",
    "plt.show\n",
    "if save_output:\n",
    "    plt.savefig(os.path.join(plots_path, \"4_imputation_differences.png\"), **config[\"plots\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8dc042",
   "metadata": {},
   "source": [
    "## 4. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clustering\n",
    "clusters_reconstr = []\n",
    "for run in range(config[\"RQ2\"][\"runs\"]):\n",
    "    dbscan_reconstr = DBSCAN(**config[\"DBSCAN\"]).fit(features_reconstr[run])\n",
    "    clusters_reconstr.append(dbscan_reconstr.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681f668",
   "metadata": {},
   "source": [
    "### 4.1. Plot of clustering based on best model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55475f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot new clustering\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(features_reconstr[0].iloc[:, 0], features_reconstr[0].iloc[:, 1], features_reconstr[0].iloc[:, 2], c=clusters_reconstr[0])\n",
    "plt.gca().update(dict(title='DBSCAN with reconstructed data', xlabel='F1', ylabel='F2', zlabel='F3'))\n",
    "plt.show()\n",
    "if save_output:\n",
    "    plt.savefig(os.path.join(plots_path, \"3_DBSCAN_imputed_data.png\"), **config[\"plots\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c79be7",
   "metadata": {},
   "source": [
    "## 5. Cluster Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fa46e",
   "metadata": {},
   "source": [
    "### 5.1 Feature Distributions\n",
    "#### 5.1.1 Distr. Plots based on best model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7be97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions\n",
    "bins = len(astropy.stats.freedman_bin_width(features[feature], return_bins=True)[1])\n",
    "n_feat = len(features_best.columns)\n",
    "\n",
    "fig, (ax) = plt.subplots(n_feat, 2, figsize=(n_feat*2,6))\n",
    "for n, feature in enumerate(features_best.columns):\n",
    "    ax[n, 0].hist(features[feature], bins=bins)\n",
    "    ax[n, 0].set_title(f\"{feature}_orig\")\n",
    "    ax[n, 1].hist(features_best[feature], bins=bins)\n",
    "    ax[n, 1].set_title(f\"{feature}_filled\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "if save_output:\n",
    "    plt.savefig(os.path.join(plots_path, \"feature distributions.png\"), **config[\"plots\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee19b7",
   "metadata": {},
   "source": [
    "#### 5.1.2 KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed653ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Divergence\n",
    "def kl_divergence(feature1, feature2):\n",
    "    \n",
    "    # calculate probability distributions\n",
    "    def probability_dist(x, y, bins):\n",
    "        # transforms 2 series into their probability distributions\n",
    "        # histogram\n",
    "        hist_xy = np.histogram2d(x, y, bins=bins)[0]\n",
    "\n",
    "        # compute marginals\n",
    "        hist_xy = hist_xy + sys.float_info.min # prevent division with 0\n",
    "        hist_xy = hist_xy / np.sum(hist_xy)\n",
    "        hist_x = np.sum(hist_xy, axis=1)\n",
    "        hist_y = np.sum(hist_xy, axis=0)\n",
    "        return(hist_x, hist_y)\n",
    "\n",
    "    # calculate kl divergence\n",
    "    bins = len(astropy.stats.freedman_bin_width(feature1, return_bins=True)[1])\n",
    "    distr1, distr2 = probability_dist(feature1, feature2, bins)\n",
    "    return torch.nn.functional.kl_div(torch.tensor(distr1).log(), torch.tensor(distr2), log_target=False, reduction='sum').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"kl_divergence\"] = {}\n",
    "\n",
    "# save kl divergence\n",
    "for run in range(config[\"RQ2\"][\"runs\"]):\n",
    "    for feature in features_to_predict:\n",
    "        output[\"kl_divergence\"][f\"run{run}_{feature}\"] = kl_divergence(features[feature], features_reconstr[run][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594a839",
   "metadata": {},
   "source": [
    "### 5.2 Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fa0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "output[\"silhouette_scores\"] = {}\n",
    "output[\"silhouette_scores\"][\"orig_data\"] = float(silhouette_score(features, np.ravel(clusters)))\n",
    "\n",
    "for run in range(config[\"RQ2\"][\"runs\"]):\n",
    "    output[\"silhouette_scores\"][f\"run{run}\"] = float(silhouette_score(features_reconstr[run], clusters_reconstr[run]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcc30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10d244",
   "metadata": {},
   "source": [
    "### 5.3. Outlierness\n",
    "#### 5.3.1 Global outliers (considering all features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers detected as noise by DBSCAN\n",
    "output[\"cluster_noise\"] = {}\n",
    "output[\"cluster_noise\"][f\"original\"] = sum(clusters_orig == -1)\n",
    "\n",
    "for run in range(config[\"RQ2\"][\"runs\"]):\n",
    "        output[\"cluster_noise\"][f\"run{run}\"] = sum(clusters_reconstr[run] == -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dae151",
   "metadata": {},
   "source": [
    "#### 5.3.1 Local outliers (outlierness within one feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot global\n",
    "\n",
    "# plot local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596e29e",
   "metadata": {},
   "source": [
    "### 5.4. Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17986663",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, \"w\") as f:\n",
    "            toml.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79779e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
