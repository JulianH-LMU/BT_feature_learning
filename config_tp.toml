[general]
experiment_name = ""
experiment_variable = "optimizer_params"
missing_percent = [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
missing_value_mask = -1000
random_seed = 123456
runs = 5

[outlier_definition]
quantiles = [0.1, 0.9]

[DBSCAN]
eps = ""
min_samples = ""

[RQ2]
pred_features = 1

[regressor_param]
#optimizer_fn = torch.optim.Adam
#scheduler_fn = torch.optim.lr_scheduler.StepLR
#scheduler_params = {'step_size': 0.2, 'gamma': 0.9}
#mask_type = "entmax"
optimizer_params = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0] 		# only used for varying learning rates!

[training]
train_val_split = 0.75
cl_weights = [1,1,0] 							# Explanation: [RMSE, KLDiv, Cluster-Mean-Loss]

[model_param]
max_epochs = [500, 400, 300, 250, 200, 150, 100]
patience = [150, 125, 100, 75, 50, 50, 50]
eval_metric = ["custom_loss"]

[plots]
dpi = 300